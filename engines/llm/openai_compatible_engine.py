"""OpenAIåè®®å…¼å®¹çš„å¤§è¯­è¨€æ¨¡å‹å¼•æ“"""
from openai import AsyncOpenAI
from engines.base import BaseEngine
from loguru import logger
from typing import AsyncGenerator, List, Dict


class OpenAICompatibleEngine(BaseEngine):
    """OpenAIåè®®å…¼å®¹çš„LLMå¼•æ“ï¼ˆæ”¯æŒDeepSeekç­‰ï¼‰"""

    def __init__(
        self,
        api_base: str,
        api_key: str,
        model: str,
        system_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 500
    ):
        """
        åˆå§‹åŒ–LLMå¼•æ“

        Args:
            api_base: APIåŸºç¡€URL
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_base = api_base
        self.api_key = api_key
        self.model = model
        self.system_prompt = system_prompt
        self.temperature = temperature
        self.max_tokens = max_tokens

        self.client = None
        self.conversation_history: List[Dict[str, str]] = []

    async def initialize(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            logger.info(f"LLMå¼•æ“å·²åˆå§‹åŒ–: {self.model}")
            logger.debug(f"API Base: {self.api_base}")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–LLMå¼•æ“å¤±è´¥: {e}")
            raise

    async def chat_stream(self, message: str) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯

        Yields:
            ç”Ÿæˆçš„token
        """
        if not self.client:
            raise RuntimeError("LLMå¼•æ“æœªåˆå§‹åŒ–")

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append({"role": "user", "content": message})

        # æ„é€ æ¶ˆæ¯
        messages = [
            {"role": "system", "content": self.system_prompt},
            *self.conversation_history
        ]

        try:
            # æµå¼è¯·æ±‚
            full_response = ""

            logger.debug(f"å‘èµ·LLMè¯·æ±‚: {message[:50]}...")

            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    full_response += token
                    yield token

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.conversation_history.append({"role": "assistant", "content": full_response})

            logger.debug(f"ğŸ¤– LLMå›å¤: {full_response}")

        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            # ç”Ÿæˆé”™è¯¯æç¤º
            error_msg = "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚"
            self.conversation_history.append({"role": "assistant", "content": error_msg})
            yield error_msg

    async def chat_with_tools(
        self,
        messages: List[Dict[str, str]],
        tools: List[Dict] = None
    ) -> Dict:
        """
        æ”¯æŒå·¥å…·è°ƒç”¨çš„å¯¹è¯ï¼ˆéæµå¼ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼ï¼‰

        Returns:
            å“åº”å­—å…¸ï¼ŒåŒ…å« content æˆ– tool_calls
        """
        if not self.client:
            raise RuntimeError("LLMå¼•æ“æœªåˆå§‹åŒ–")

        try:
            logger.debug(f"å‘èµ·LLMè¯·æ±‚ï¼ˆæ”¯æŒå·¥å…·ï¼‰: {len(messages)} æ¡æ¶ˆæ¯")

            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
            }

            # å¦‚æœæä¾›äº†å·¥å…·å®šä¹‰ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if tools:
                request_params["tools"] = tools
                request_params["tool_choice"] = "auto"

            response = await self.client.chat.completions.create(**request_params)

            choice = response.choices[0]
            message = choice.message

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if hasattr(message, 'tool_calls') and message.tool_calls:
                logger.debug(f"ğŸ”§ LLMè¯·æ±‚è°ƒç”¨å·¥å…·: {[tc.function.name for tc in message.tool_calls]}")
                return {
                    "type": "tool_calls",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                        for tc in message.tool_calls
                    ]
                }
            else:
                # æ™®é€šæ–‡æœ¬å“åº”
                content = message.content or ""
                logger.debug(f"ğŸ¤– LLMå›å¤: {content}")
                return {
                    "type": "text",
                    "content": content
                }

        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            raise

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
        logger.debug("å¯¹è¯å†å²å·²æ¸…ç©º")

    def get_history_length(self) -> int:
        """è·å–å¯¹è¯å†å²é•¿åº¦"""
        return len(self.conversation_history)

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.client:
            await self.client.close()
        self.conversation_history = []
        logger.info("LLMå¼•æ“å·²æ¸…ç†")
