"""OpenAIåè®®å…¼å®¹çš„å¤§è¯­è¨€æ¨¡å‹å¼•æ“"""
from openai import AsyncOpenAI
from engines.base import BaseEngine
from loguru import logger
from typing import AsyncGenerator, List, Dict


class OpenAICompatibleEngine(BaseEngine):
    """OpenAIåè®®å…¼å®¹çš„LLMå¼•æ“ï¼ˆæ”¯æŒDeepSeekç­‰ï¼‰"""

    def __init__(
        self,
        api_base: str,
        api_key: str,
        model: str,
        system_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 500
    ):
        """
        åˆå§‹åŒ–LLMå¼•æ“

        Args:
            api_base: APIåŸºç¡€URL
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_base = api_base
        self.api_key = api_key
        self.model = model
        self.system_prompt = system_prompt
        self.temperature = temperature
        self.max_tokens = max_tokens

        self.client = None
        self.conversation_history: List[Dict[str, str]] = []

    async def initialize(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            logger.info(f"LLMå¼•æ“å·²åˆå§‹åŒ–: {self.model}")
            logger.debug(f"API Base: {self.api_base}")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–LLMå¼•æ“å¤±è´¥: {e}")
            raise

    async def chat_stream(self, message: str) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯

        Yields:
            ç”Ÿæˆçš„token
        """
        if not self.client:
            raise RuntimeError("LLMå¼•æ“æœªåˆå§‹åŒ–")

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append({"role": "user", "content": message})

        # æ„é€ æ¶ˆæ¯
        messages = [
            {"role": "system", "content": self.system_prompt},
            *self.conversation_history
        ]

        try:
            # æµå¼è¯·æ±‚
            full_response = ""

            logger.debug(f"å‘èµ·LLMè¯·æ±‚: {message[:50]}...")

            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    full_response += token
                    yield token

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.conversation_history.append({"role": "assistant", "content": full_response})

            logger.debug(f"ğŸ¤– LLMå›å¤: {full_response}")

        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            # ç”Ÿæˆé”™è¯¯æç¤º
            error_msg = "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚"
            self.conversation_history.append({"role": "assistant", "content": error_msg})
            yield error_msg

    async def chat_with_tools(
        self,
        messages: List[Dict[str, str]],
        tools: List[Dict] = None
    ) -> Dict:
        """
        æ”¯æŒå·¥å…·è°ƒç”¨çš„å¯¹è¯ï¼ˆéæµå¼ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼ï¼‰

        Returns:
            å“åº”å­—å…¸ï¼ŒåŒ…å« content æˆ– tool_calls
        """
        if not self.client:
            raise RuntimeError("LLMå¼•æ“æœªåˆå§‹åŒ–")

        try:
            logger.debug(f"å‘èµ·LLMè¯·æ±‚ï¼ˆæ”¯æŒå·¥å…·ï¼‰: {len(messages)} æ¡æ¶ˆæ¯")

            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
            }

            # å¦‚æœæä¾›äº†å·¥å…·å®šä¹‰ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if tools:
                request_params["tools"] = tools
                request_params["tool_choice"] = "auto"

            response = await self.client.chat.completions.create(**request_params)

            choice = response.choices[0]
            message = choice.message

            # Extract usage data if available
            # å¦‚æœæœ‰ usage æ•°æ®åˆ™æå–å‡ºæ¥
            usage = None
            if hasattr(response, "usage") and response.usage:
                usage = {
                    "prompt_tokens": response.usage.prompt_tokens or 0,
                    "completion_tokens": response.usage.completion_tokens or 0,
                    "total_tokens": response.usage.total_tokens or 0,
                }

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if hasattr(message, 'tool_calls') and message.tool_calls:
                logger.debug(f"ğŸ”§ LLMè¯·æ±‚è°ƒç”¨å·¥å…·: {[tc.function.name for tc in message.tool_calls]}")
                result = {
                    "type": "tool_calls",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                        for tc in message.tool_calls
                    ],
                }
            else:
                # æ™®é€šæ–‡æœ¬å“åº”
                content = message.content or ""
                logger.debug(f"ğŸ¤– LLMå›å¤: {content}")
                result = {
                    "type": "text",
                    "content": content,
                }

            if usage:
                result["usage"] = usage
            return result

        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            raise

    async def chat_with_tools_stream(
        self,
        messages: List[Dict[str, str]],
        tools: List[Dict] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        æ”¯æŒå·¥å…·è°ƒç”¨çš„æµå¼å¯¹è¯

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼ï¼‰

        Yields:
            å“åº”å­—å…¸:
            {"type": "text_delta", "content": "..."}
            {"type": "tool_calls", "tool_calls": [...]}
            {"type": "usage", "usage": {...}}
        """
        if not self.client:
            raise RuntimeError("LLMå¼•æ“æœªåˆå§‹åŒ–")

        try:
            logger.debug(f"å‘èµ·LLMæµå¼è¯·æ±‚ï¼ˆæ”¯æŒå·¥å…·ï¼‰: {len(messages)} æ¡æ¶ˆæ¯")

            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "stream": True,
            }

            if tools:
                request_params["tools"] = tools
                request_params["tool_choice"] = "auto"
                # Add stream_options for usage if supported (OpenAI standard)
                # è‹¥æœåŠ¡ç«¯æ”¯æŒï¼Œå¯å¼€å¯ stream_options è¿”å› usageï¼ˆOpenAI æ ‡å‡†ï¼‰
                # request_params["stream_options"] = {"include_usage": True}
                # ç¤ºä¾‹ï¼šå¼€å¯å¢é‡æµä¸­çš„ usage ç»Ÿè®¡

            stream = await self.client.chat.completions.create(**request_params)

            tool_calls_buffer = {}  # index -> dict
            # å·¥å…·è°ƒç”¨å¢é‡ç¼“å†²ï¼šç´¢å¼• -> è°ƒç”¨å¯¹è±¡

            async for chunk in stream:
                # Handle usage if present in the chunk (some providers send it in the last chunk)
                # å¤„ç† chunk ä¸­æºå¸¦çš„ usageï¼ˆéƒ¨åˆ†æœåŠ¡å•†ä¼šåœ¨æœ€åä¸€ä¸ª chunk è¿”å›ï¼‰
                if hasattr(chunk, "usage") and chunk.usage:
                    yield {
                        "type": "usage",
                        "usage": {
                            "prompt_tokens": chunk.usage.prompt_tokens,
                            "completion_tokens": chunk.usage.completion_tokens,
                            "total_tokens": chunk.usage.total_tokens,
                        }
                    }

                if not chunk.choices:
                    continue

                delta = chunk.choices[0].delta

                # 1. Text Content
                # 1. æ–‡æœ¬å¢é‡
                if delta.content:
                    yield {"type": "text_delta", "content": delta.content}

                # 2. Tool Calls
                # 2. å·¥å…·è°ƒç”¨å¢é‡
                if delta.tool_calls:
                    for tc in delta.tool_calls:
                        idx = tc.index
                        if idx not in tool_calls_buffer:
                            tool_calls_buffer[idx] = {
                                "id": "",
                                "function": {"name": "", "arguments": ""}
                            }

                        entry = tool_calls_buffer[idx]
                        if tc.id:
                            entry["id"] += tc.id
                        if tc.function:
                            if tc.function.name:
                                entry["function"]["name"] += tc.function.name
                            if tc.function.arguments:
                                entry["function"]["arguments"] += tc.function.arguments

            # End of stream logic
            # æµç»“æŸåçš„æ”¶å°¾é€»è¾‘
            if tool_calls_buffer:
                # Convert buffer to list
                # æŠŠç¼“å†²å­—å…¸è½¬æ¢ä¸ºæœ‰åºåˆ—è¡¨
                tool_calls = []
                for idx in sorted(tool_calls_buffer.keys()):
                    entry = tool_calls_buffer[idx]
                    tool_calls.append({
                        "id": entry["id"],
                        "name": entry["function"]["name"],
                        "arguments": entry["function"]["arguments"]
                    })

                logger.debug(f"ğŸ”§ LLMæµå¼è°ƒç”¨å·¥å…·: {[t['name'] for t in tool_calls]}")
                yield {
                    "type": "tool_calls",
                    "tool_calls": tool_calls
                }

            # Note: For text responses, we've already yielded all deltas.
            # æ³¨æ„ï¼šæ–‡æœ¬å“åº”çš„æ‰€æœ‰å¢é‡å·²ç»åœ¨ä¸Šé¢é€æ­¥äº§å‡ºã€‚
            # The AgentCore will responsible for accumulating them for history.
            # åç»­ç”± AgentCore è´Ÿè´£æŠŠè¿™äº›å¢é‡æ‹¼æ¥å¹¶å†™å…¥å†å²ã€‚

        except Exception as e:
            logger.error(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
        logger.debug("å¯¹è¯å†å²å·²æ¸…ç©º")

    def get_history_length(self) -> int:
        """è·å–å¯¹è¯å†å²é•¿åº¦"""
        return len(self.conversation_history)

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.client:
            await self.client.close()
        self.conversation_history = []
        logger.info("LLMå¼•æ“å·²æ¸…ç†")
